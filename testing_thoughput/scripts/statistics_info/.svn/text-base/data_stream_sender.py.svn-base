#!/usr/bin/python
#
# The purpose of this python script is to collect metrics for hash join node.
# Given the impala query profile file as the input, 
# this script iterate through the file;
# skip 'averaged fragment' part; 
# collect all hash join related metrics from the 'instance fragment' part;
#
# The output of the script is a comma seperated csv file;
#
import sys
import os
import string
import re
import csv


from scripts.utility import *
from statistics import *
def collect_statistics(profile_path, output_file_path, output_dir_path="."):

	keys = ['id',                          # index 0
			'BytesSent',                   # index 1
            'NetworkThroughput(*)',        # index 2
           	'OverallThroughput',           # index 3
           	'PeakMemoryUsage',             # index 4           	 
           	'SerializeBatchTime',          # index 5
           	'ThriftTransmitTime(*)',       # index 6
           	'UncompressedRowBatchSize'     # index 7
		    ]

	func_map = {}
	func_map['BytesSent'] = get_count
	func_map['NetworkThroughput(*)'] = get_count
	func_map['OverallThroughput'] = get_count
	func_map['PeakMemoryUsage'] = get_count
	func_map['SerializeBatchTime'] = get_time
	func_map['ThriftTransmitTime(*)'] = get_count
	func_map['UncompressedRowBatchSize'] = get_count
	
	record = []
	
	average_fragment_metrics_skipped = False
	within_data_stream_sender_entry = False
	within_plan_fragment_entries = False
	
	node_id = 0
	geomean_serialization_bytes_per_ns = 1.0
	geomean_decompression_ratio = 1.0

	sample_decompression_ratio = []
	sample_serialization_bytes_per_ns = []

	decompression_ratio_per_node_map = {}
	serialization_bytes_per_ns_per_node_map = {}

	for line in open(profile_path):
		stripped_line = line.strip();

		if stripped_line.startswith("Estimated Per-Host Requirements:"):
			within_plan_fragment_entries = True
		elif stripped_line.startswith("Estimated Per-Host Mem:"):
			within_plan_fragment_entries = False							
		elif stripped_line.startswith("Instance") and not average_fragment_metrics_skipped:
			average_fragment_metrics_skipped = True
		elif stripped_line.startswith("DataStreamSender (dst_id=") and average_fragment_metrics_skipped:
			curr_id = get_exec_node_id(stripped_line)
			record.append(curr_id)
			within_data_stream_sender_entry = True
		elif average_fragment_metrics_skipped and within_data_stream_sender_entry:

			key = get_label(stripped_line)
			
			if key != '' and key in func_map:
				value = func_map[key](stripped_line)
				record.append(value)

				if key == 'UncompressedRowBatchSize':
					
					if float(record[5]) > 0:
						serialization_bytes_per_ns = float(record[1]) / float(record[5])
					else:
						serialization_bytes_per_ns = 0
					record.append(serialization_bytes_per_ns)
					sample_serialization_bytes_per_ns.append(serialization_bytes_per_ns)

					if curr_id not in serialization_bytes_per_ns_per_node_map.keys():
						serialization_bytes_per_ns_per_node_map[curr_id] = []
					serialization_bytes_per_ns_per_node_map[curr_id].append(serialization_bytes_per_ns)

					if float(record[1]) > 0:
						decompression_ratio = float(record[7]) / float(record[1]) 
					else:
						decompression_ratio = 0
					record.append(decompression_ratio)					
					sample_decompression_ratio.append(decompression_ratio)

					if curr_id not in decompression_ratio_per_node_map.keys():
						decompression_ratio_per_node_map[curr_id] = []
					decompression_ratio_per_node_map[curr_id].append(decompression_ratio)

					within_data_stream_sender_entry = False
					
					record = []
					
		# enf if
	# end for		

	with open(output_file_path, 'a', newline='') as csv_file:
		csv_writer = csv.writer(csv_file, delimiter=',')	
		for key in serialization_bytes_per_ns_per_node_map.keys():
			csv_writer.writerow(['ROW_BATCH_SER_COST', key, '{0:.8f}'.format(average(serialization_bytes_per_ns_per_node_map[key]))])
		for key in decompression_ratio_per_node_map.keys():
			csv_writer.writerow(['DECOMPRESSION_RATIO', key, '{0:.8f}'.format(average(decompression_ratio_per_node_map[key]))])

	global produce_combined_csv
	if 'produce_combined_csv' not in globals():		
		produce_combined_csv = False
	global csv_global_writer
	if produce_combined_csv:
		global start_execution_timestamp
		with open(os.path.join(output_dir_path, 'data-stream-sender-all-' + start_execution_timestamp + '.csv'), 'a', newline='') as global_csv_file:
			csv_global_writer = csv.writer(global_csv_file, delimiter=',') 
			import operator
			sorted_serialization_bytes_per_ns_per_node_map = sorted(serialization_bytes_per_ns_per_node_map.items(), key=operator.itemgetter(0))
			for item in sorted_serialization_bytes_per_ns_per_node_map:
				csv_global_writer.writerow([os.path.basename(output_file_path), 'ROW_BATCH_SER_COST', item[0], '{0:.8f}'.format(average(item[1]))])					
			sorted_decompression_ratio_per_node_map = sorted(decompression_ratio_per_node_map.items(), key=operator.itemgetter(0))
			for item in sorted_decompression_ratio_per_node_map:
				csv_global_writer.writerow([os.path.basename(output_file_path), 'DECOMPRESSION_RATIO', item[0], '{0:.8f}'.format(average(item[1]))])	

if __name__ == '__main__':
	import shutil
	import platform
	print(platform.python_version())

	global produce_combined_csv
	produce_combined_csv = True

	global start_execution_timestamp
	
	query_name = ''
	for task in ['']:

		start_execution_timestamp = str(current_milli_time())
		
		path_prefix = r'W:\junliu\Benchmark\Impala\Baseline (Parquet)\profiles'
		profile = os.path.join(path_prefix, task, query_name)
		output = os.path.join(path_prefix, 'data_stream_sender', task)
		
		if os.path.exists(output):
			shutil.rmtree(output)
			print('remove folder: %s, %s' % (output, os.path.exists(output)))

		sys.argv = ['data_stream_sender.py', profile]

		if len(sys.argv) < 2 or not os.path.exists(sys.argv[1]):
			print("usage: data_stream_sender.py <impala profile file path or directory>")
			sys.exit()

		handle_profiles(collect_statistics, os.path.normpath(profile), os.path.normpath(output))

		print('finish {0}'.format(task))
